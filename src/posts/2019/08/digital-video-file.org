#+title: 视频文件简单了解
#+date: 2019-08-18
#+index: 视频文件简单了解
#+tags: Video-file

#+macro: frame 帧
#+macro: interlace-frame 交错帧
#+macro: progressive-frame 渐近帧
#+macro: format 文件格式
#+macro: codec 编码解码器
#+macro: filter 滤镜
#+macro: bitrate 比特率

#+begin_abstract
因为临时的工作需要,要处理视频无法在 =ckplayer= 上面播放的问题,久违地使用了一下 =ffmpeg=.

在17年的时候就买了 =ffmpeg= 官网上推荐的 =ffmpeg basics= 来学习 =ffmpeg=,那个时候也是因为工作需求,

但是那个时候我还没有开始写 =blog=,所以这段时间会重新学习一边,顺便写些东西.

(从7.25日开始忙着下班玩火纹:风花雪月,已经偷懒了一段时间了,看来要把学习和锻炼像吃饭睡觉一样作为日常生活必选项).

复习 =ffmpeg= 之前先了解以下视频文件的一些基本概念,否则读 =ffmpeg basics= 会懵,我觉得这本书唯一的缺点就是没有先介绍视频文件,

对 =codec= ({{{codec}}}), =format= ({{{format}}})等基础概念的不理解会造成对 =ffmpeg= 的工作流程的不理解.
#+end_abstract

在深入了解视频一些技术参数之前先讲一下视频文件的相关知识.很多人会把{{{codec}}}和{{{format}}}搞混,其实两者是不同的东西.

*** 视频编码和解码

通常录制完的视频都需要做压缩处理的,否则文件大小会十分庞大不方便传输和储存.压缩也就是视频编码,编码的组件叫做 =encoder=,

对应压缩就有解压,也就是视频解码,解码的组件叫做 =decoder=,两个组件组合起来就是一个叫做{{{codec}}}的软件/硬件.

值得一提的是压缩(基本都)是失真的,也就是说后面对视频进行解码也是不能得到和原来一样的视频.

而视频转码的概念也就是换种编码方式:先用旧的编码方式解码,然后用另外一种方式给解码后的数据进行编码.


*** 视频{{{format}}}以及视频文件构成

视频编码后并非能马上储存在文件系统上的,一个视频文件是由视频数据和音频数据组成的,编码后数据要放到一个容器 =F= 里面,

而容器 =F= 又是由用于存放视频数据的子容器 =V= 和用于存放音频数据的子容器 =A= 组成的,容器 =F= 还可能包含视频同步信息,视频文件元信息以及字幕.

容器 =F= 就是我们说的视频{{{format}}}, =mp4=, =mkv= 等等就是常见的视频文件容器;

子容器 =V= 就是视频编码格式/视频压缩格式(=video coding format/video compression format=),比如 =H.264=, =MPEG4-2= 等等,比如对应 =H.264= 的 =codec= 叫做 =H264=.

子容器 =A= 就是音频编码格式/音频压缩格式(=audio coding format/audio compression format=),比如大名鼎鼎的 =mp3=.

一种的视频{{{format}}}由特定的视频编码格式以及音频编码格式组成,比如 =mp4= 可以由 =H.264= 以及 =mp3= 两种格式组成,也可以是由 =MPEG-1= 和 =MP3= 两种格式组成,各种常见的格式可以看看这[[https://en.m.wikipedia.org/wiki/Video_file_format][维基页]].

正如上个小节提到的,视频是要先经过编码后再以某种格式把视频数据和音频数据封装成一个文件进行储存的,负责格式封装的组件叫做 =muxer=,同样对应封装的还有分离/分流,负责这块的逐渐叫做 =demuxer=.

(我不清楚 =muxer= 和 =demuxer= 是否只是 =ffmpeg= 的概念,但因为这篇文章是学习 =ffmpeg= 先导文,所以我就这么写了).

所以很多人以为转视频编码就要转视频{{{format}}},这是错误的.


*** 视频数据之{{{frame}}}

    视频数据实际上就是一系列的连续图片,也就是动画实现的原理,每一张图片叫做{{{frame}}}(=frame=).

    常说的一个视频每秒XX{{{frame}}},也就是指这个视频的{{{frame}}}率为 =XX fps= 或者写作 =XX f/s=.

    =fps= 或者 =f/s= 的缩写是 =frames per second=.

    比如一个视频30fps,那么就是这个视频一秒播放30张图片,所以在图片质量不变的情况下改变{{{frame}}}率可以改变视频大小.

    一{{{frame}}}就是一张位图(=bitmap=),位图是一个亮度和像素数量两纬数组表示的,而像素只有一个属性,就是颜色.

    一个像素的颜色是由固定数量的位(=bits=)表示的,这个数量叫做色彩深度(=color depth=),位越多,可用的颜色越多,24位色彩深度就有 *2的24次方* 种颜色可用.

    {{{frame}}}的颜色深度也就是视频的颜色深度.

    {{{frame}}}有两种: {{{interlace-frame}}}(=interlace frame=) 以及{{{progressive-frame}}}(=progressive frame=).


**** {{{interlace-frame}}}

    采用{{{interlace-frame}}}的视频叫做交错视频(=interlaced video=),每一完整{{{frame}}}都是由一张图片的两半组成的,

    第一半包含了图片的奇数行(=odd-numbered lines=),第二半就包含了图片的偶数行(=even-numbered lines=).

    这样的一半叫做域(=filed=),也叫做{{{interlace-frame}}},两个连续的域构成一个完整的{{{frame}}}.一个30fps交错视频每秒播放60个域.

    每秒60域可以写作 60 =fields per second=,缩写是 =60i fps=,而不是 =60fps=,注意 =i=, =i= 表示交错.


**** {{{progressive-frame}}}


     和{{{interlace-frame}}}不同的是,{{{progressive-frame}}}是逐行/连续扫瞄完一张图片的.

     平常说的一个视频的每秒XX{{{frame}}}就是指的每秒XX{{{progressive-frame}}},所以 =30fps= 也就是 =30p fps=.


**** {{{interlace-frame}}}与{{{progressive-frame}}}

     两者的关系就是: =60i fps = 30p fps= 或者说 =60i fps = 30fps=.交错视频和非交错视频是可以相互转换的.

     大部份视频设备都是交错的,这是为了减少传输带来的带宽压力以及阻止闪烁,而大部份视频都是非交错视频,所以转换是必须的.

     正如上面{{{interlace-frame}}}章节中说到的,一个完整会被隔行分成两半,同样,也可以把非交错视频的每个{{{frame}}}按照交错视频那样分成两半.

     这样分出来的一半叫做渐近段{{{frame}}}(=progressive segmented frame=,缩写 =PsF/sF/SF=),技术上来说,{{{interlace-frame}}}和渐近段{{{frame}}}是一样的.

     与原生交错视频不同的是,两个段{{{frame}}}之间是没有动作补间(=motion=)的.


*** 视频数据之{{{bitrate}}}
